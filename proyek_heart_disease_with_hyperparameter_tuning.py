# -*- coding: utf-8 -*-
"""Proyek Heart Disease with Hyperparameter Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xjyf9wmX-2chY1oOv4KiS7SVNxiwzoZ5
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load the Heart Disease dataset
dataset = 'https://storage.googleapis.com/dqlab-dataset/heart_disease.csv'
heart_data = pd.read_csv(dataset)

# Preprocessing and cleaning
# Handle missing values
heart_data = heart_data.dropna()

# Standardize the features
scaler = StandardScaler()
heart_data_scaled = scaler.fit_transform(heart_data.drop('target', axis=1))

# Perform PCA for dimensionality reduction
feature_number = len(heart_data_scaled[0])
pca = PCA(n_components=feature_number)

# Fit PCA with dataset
pca.fit(heart_data_scaled)
pca = PCA(n_components=9)
heart_data_reduced = pca.fit_transform(heart_data_scaled)

# Load the Heart Disease dataset
X = heart_data_reduced
y = heart_data['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Selection using LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier with random_state = 42
models = [
    ('Logistic Regression', LogisticRegression(random_state=42)),
    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Random Forest', RandomForestClassifier(random_state=42)),
    ('Neural Network', MLPClassifier(random_state=42))
]

# Evaluate each model using cross-validation
best_model = None
best_accuracy = 0
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = model

    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print()

print(f"Best Model: {best_model.__class__.__name__}")
print(f"Best Accuracy: {best_accuracy:.2f}")

# Model Tuning
# Define the parameter grid for tuning
param_grid = {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}

# Perform hyperparameter tuning using GridSearchCV for Logistic Regression with defined parameter and cv = 5
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best model and its parameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

# Evaluate the best model
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("")
print("Best Model: Logistic Regression")
print("Best Parameters:", best_params)
print(f"Accuracy: {accuracy:.2f}")